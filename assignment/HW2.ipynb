{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: Data Wrangling\n",
    "## Student: Vicky\n",
    "## Date: January 27, 2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Cleaning Variables with Common Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1: Numeric Variable - Airbnb Price\n",
    "Clean the Price variable and explain choices made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Airbnb data\n",
    "airbnb = pd.read_csv('airbnb_hw.csv')\n",
    "\n",
    "print(f\"Dataset shape: {airbnb.shape}\")\n",
    "print(f\"\\nFirst few Price values:\")\n",
    "print(airbnb['Price'].head(20))\n",
    "print(f\"\\nData type: {airbnb['Price'].dtype}\")\n",
    "print(f\"\\nSample of Price values:\")\n",
    "print(airbnb['Price'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the Price variable\n",
    "# The Price column appears to be stored as strings of integers\n",
    "# Convert to numeric, handling any non-numeric values\n",
    "\n",
    "airbnb['Price_cleaned'] = pd.to_numeric(airbnb['Price'], errors='coerce')\n",
    "\n",
    "# Check results\n",
    "print(f\"Missing values after cleaning: {airbnb['Price_cleaned'].isna().sum()}\")\n",
    "print(f\"Proportion missing: {airbnb['Price_cleaned'].isna().sum() / len(airbnb):.3f}\")\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(airbnb['Price_cleaned'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of choices:**\n",
    "\n",
    "- The Price variable was stored as an object (string) type but contained numeric values\n",
    "- Used `pd.to_numeric()` with `errors='coerce'` to convert strings to numbers\n",
    "- This approach handles any non-numeric values by converting them to NaN rather than throwing an error\n",
    "- **Missing values**: 181 out of 30,478 records (0.6%) could not be converted to numeric values\n",
    "- The cleaned data shows prices ranging from $10 to $999, with a median of $125\n",
    "- No obvious data entry errors (like negative prices or implausibly high values)\n",
    "\n",
    "The small proportion of missing values suggests the data is generally well-maintained. The 181 missing values likely represent records where price information was truly unavailable rather than systematic data quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2: Categorical Variable - Police Use of Force\n",
    "Clean `subject_injury` and analyze missing patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Minnesota police data\n",
    "mn_police = pd.read_csv('mn_police_use_of_force.csv')\n",
    "\n",
    "print(f\"Dataset shape: {mn_police.shape}\")\n",
    "print(f\"\\nValue counts for subject_injury:\")\n",
    "print(mn_police['subject_injury'].value_counts(dropna=False))\n",
    "print(f\"\\nProportion missing: {mn_police['subject_injury'].isna().sum() / len(mn_police):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data appears already clean with Yes/No/NaN values\n",
    "# Create a copy for consistency\n",
    "mn_police['subject_injury_cleaned'] = mn_police['subject_injury'].copy()\n",
    "\n",
    "print(\"Cleaned value counts:\")\n",
    "print(mn_police['subject_injury_cleaned'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing patterns by force_type\n",
    "print(\"\\nCross-tabulation of force_type and subject_injury:\")\n",
    "crosstab = pd.crosstab(\n",
    "    mn_police['force_type'], \n",
    "    mn_police['subject_injury_cleaned'].fillna('Missing'), \n",
    "    margins=True\n",
    ")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate proportion missing by force type\n",
    "print(\"\\nProportion missing by force_type (sorted):\")\n",
    "missing_by_type = mn_police.groupby('force_type')['subject_injury_cleaned'].apply(\n",
    "    lambda x: x.isna().sum() / len(x)\n",
    ").sort_values(ascending=False)\n",
    "print(missing_by_type)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_by_type.plot(kind='barh')\n",
    "plt.xlabel('Proportion Missing')\n",
    "plt.title('Proportion of Missing subject_injury Data by Force Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "**Proportion missing:** 76.2% (9,848 out of 12,925 records)\n",
    "\n",
    "**Is this a concern?** Yes, this is a major concern. The high missing rate severely limits our ability to analyze injury patterns.\n",
    "\n",
    "**Missing data patterns:**\n",
    "\n",
    "1. **Systematic missingness by force type:**\n",
    "   - \"Less Lethal\" and \"Maximal Restraint Technique\": 100% missing\n",
    "   - \"Chemical Irritant\": 89% missing\n",
    "   - \"Taser\": 75% missing  \n",
    "   - \"Bodily Force\" (most common, 73% of incidents): 75% missing\n",
    "   - \"Police K9 Bite\" and \"Gun Point Display\": Lower missing rates (~26-40%)\n",
    "\n",
    "2. **This suggests Missing Not At Random (MNAR):**\n",
    "   - Injury data appears more likely to be recorded for certain force types\n",
    "   - More severe or visible force types (K9 bites, gun point) have better documentation\n",
    "   - Routine force types (bodily force) have poor documentation\n",
    "\n",
    "3. **Possible explanations:**\n",
    "   - Injuries may be recorded more consistently when:\n",
    "     - The force used is more severe or visible\n",
    "     - Medical attention is required\n",
    "     - A complaint is filed\n",
    "   - For routine bodily force, injury status may not be systematically documented\n",
    "\n",
    "4. **Implications:**\n",
    "   - Cannot reliably estimate overall injury rates from this data\n",
    "   - Comparing injury rates across force types would be biased\n",
    "   - The 76% missing rate means we only have complete information for about 1 in 4 incidents\n",
    "   - Any analysis should be limited to complete cases with appropriate caveats\n",
    "\n",
    "This is a clear example of why understanding missing data patterns is crucial before analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3: Dummy Variable - Pretrial Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrial data\n",
    "pretrial = pd.read_parquet('justice_data.parquet')\n",
    "\n",
    "print(f\"Dataset shape: {pretrial.shape}\")\n",
    "print(f\"\\nWhetherDefendantWasReleasedPretrial value counts:\")\n",
    "print(pretrial['WhetherDefendantWasReleasedPretrial'].value_counts(dropna=False))\n",
    "print(f\"\\nData type: {pretrial['WhetherDefendantWasReleasedPretrial'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the variable - convert to proper dummy variable with np.nan for missing\n",
    "# The value 9 appears to be a missing data code\n",
    "pretrial['released_pretrial_cleaned'] = pretrial['WhetherDefendantWasReleasedPretrial'].copy()\n",
    "pretrial.loc[pretrial['released_pretrial_cleaned'] == 9, 'released_pretrial_cleaned'] = np.nan\n",
    "\n",
    "print(\"Cleaned value counts:\")\n",
    "print(pretrial['released_pretrial_cleaned'].value_counts(dropna=False))\n",
    "print(f\"\\nMissing values: {pretrial['released_pretrial_cleaned'].isna().sum()}\")\n",
    "print(f\"Proportion missing: {pretrial['released_pretrial_cleaned'].isna().sum() / len(pretrial):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- The variable was coded as 0/1/9, where 9 appears to be a missing data code\n",
    "- Converted the 9 values to `np.nan` to properly represent missing data\n",
    "- This creates a clean dummy variable where:\n",
    "  - 0 = defendant was NOT released pretrial\n",
    "  - 1 = defendant WAS released pretrial\n",
    "  - NaN = status unknown\n",
    "- Only 31 cases (0.13%) have missing status, indicating good data quality\n",
    "- The vast majority (83%) of defendants were released pretrial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.4: Missing Values Not at Random - Sentence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine ImposedSentenceAllChargeInContactEvent\n",
    "print(\"ImposedSentence - checking for empty vs. populated:\")\n",
    "imposed_empty = (pretrial['ImposedSentenceAllChargeInContactEvent'].astype(str).str.strip() == '')\n",
    "\n",
    "print(f\"Empty/missing values: {imposed_empty.sum()}\")\n",
    "print(f\"Non-empty values: {(~imposed_empty).sum()}\")\n",
    "print(f\"\\nSample of non-empty ImposedSentence values:\")\n",
    "print(pretrial[~imposed_empty]['ImposedSentenceAllChargeInContactEvent'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine SentenceTypeAllChargesAtConvictionInContactEvent\n",
    "print(\"SentenceType value counts:\")\n",
    "print(pretrial['SentenceTypeAllChargesAtConvictionInContactEvent'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "# Create dictionary to interpret codes (based on typical justice system coding)\n",
    "sentence_type_labels = {\n",
    "    0: 'No sentence/Not convicted',\n",
    "    1: 'Incarceration',\n",
    "    2: 'Probation/Suspended',\n",
    "    4: 'Fine/Other non-incarcerative',\n",
    "    9: 'Unknown'\n",
    "}\n",
    "\n",
    "print(\"\\nInterpreted SentenceType:\")\n",
    "for code, label in sentence_type_labels.items():\n",
    "    count = (pretrial['SentenceTypeAllChargesAtConvictionInContactEvent'] == code).sum()\n",
    "    print(f\"  {code} ({label}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulate to understand relationship\n",
    "print(\"\\nCross-tabulation: SentenceType vs ImposedSentence (is empty):\")\n",
    "crosstab = pd.crosstab(\n",
    "    pretrial['SentenceTypeAllChargesAtConvictionInContactEvent'],\n",
    "    imposed_empty,\n",
    "    margins=True\n",
    ")\n",
    "crosstab.columns = ['Has Sentence Value', 'Empty/Missing', 'Total']\n",
    "print(crosstab)\n",
    "\n",
    "# Calculate proportions\n",
    "print(\"\\nProportion with empty ImposedSentence by SentenceType:\")\n",
    "props = pretrial.groupby('SentenceTypeAllChargesAtConvictionInContactEvent').apply(\n",
    "    lambda x: (x['ImposedSentenceAllChargeInContactEvent'].astype(str).str.strip() == '').mean(),\n",
    "    include_groups=False\n",
    ")\n",
    "for code, prop in props.items():\n",
    "    label = sentence_type_labels.get(code, 'Unknown')\n",
    "    print(f\"  {code} ({label}): {prop:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of choices:**\n",
    "\n",
    "This is a clear example of **Missing Not At Random (MNAR)** - the missingness is directly related to the value of another variable.\n",
    "\n",
    "**Key findings:**\n",
    "\n",
    "1. **Perfect association between SentenceType and ImposedSentence:**\n",
    "   - SentenceType 0, 1, 2: ImposedSentence is ALWAYS populated (100%)\n",
    "   - SentenceType 4, 9: ImposedSentence is ALWAYS empty (100%)\n",
    "\n",
    "2. **Why this makes sense:**\n",
    "   - **SentenceType 0** (\"No sentence/Not convicted\"): 8,720 cases\n",
    "     - These cases were dismissed, acquitted, or charges were dropped\n",
    "     - BUT they still have values in ImposedSentence (probably zeros or specific codes)\n",
    "   - **SentenceType 1** (\"Incarceration\"): 4,299 cases\n",
    "     - Defendant sentenced to jail/prison\n",
    "     - ImposedSentence contains the length (in days/months)\n",
    "   - **SentenceType 2** (\"Probation/Suspended\"): 914 cases\n",
    "     - Defendant received probation or suspended sentence\n",
    "     - ImposedSentence likely contains probation length\n",
    "   - **SentenceType 4** (\"Fine/Other\"): 8,779 cases\n",
    "     - Fines or other non-incarcerative sentences\n",
    "     - ImposedSentence field is empty (fine amounts likely in different field)\n",
    "   - **SentenceType 9** (\"Unknown\"): 274 cases\n",
    "     - Status unclear, no sentence data available\n",
    "\n",
    "3. **Implications:**\n",
    "   - The missing ImposedSentence values are **structural** - they're missing because those sentence types don't have associated incarceration lengths\n",
    "   - This is not a data quality problem; it's by design\n",
    "   - We should NOT impute these missing values\n",
    "   - Analysis of sentence length should be restricted to SentenceType codes where it's meaningful (0, 1, 2)\n",
    "\n",
    "4. **How to handle:**\n",
    "   - Keep the missing values as-is\n",
    "   - When analyzing sentence length, filter to `SentenceType.isin([0, 1, 2])`\n",
    "   - Document that the missingness is informative and structural\n",
    "\n",
    "This is an excellent example of why understanding the *reason* for missing data is crucial - not all missing data should be treated the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Shark Attack Data Analysis\n",
    "\n",
    "**Note:** The shark attack data file needs to be uploaded to complete this section. Once the file (GSAF5.xls or sharks.csv) is available, this section will analyze:\n",
    "- Cleaning Year and Age variables\n",
    "- Analyzing trends in attacks since 1940\n",
    "- Creating demographic breakdowns\n",
    "- Examining attack types (Provoked/Unprovoked)\n",
    "- Analyzing fatality patterns\n",
    "- Identifying white shark attack proportions\n",
    "\n",
    "*This section will be completed once the data file is uploaded.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Tidy Data Paper Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1: Abstract\n",
    "\n",
    "**What is this paper about?**\n",
    "\n",
    "This paper is about data tidying - a small but important component of data cleaning. Hadley Wickham defines \"tidy data\" as a standardized way to structure datasets that makes data manipulation, visualization, and modeling easier. The paper proposes three principles for tidy data:\n",
    "\n",
    "1. Each variable forms a column\n",
    "2. Each observation forms a row\n",
    "3. Each type of observational unit forms a table\n",
    "\n",
    "The paper demonstrates how most messy datasets violate these principles in predictable ways, and shows that a small set of tools (melting, string splitting, casting) can fix most common problems. The goal is to spend less time on data cleaning logistics and more time on actual analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2: Introduction\n",
    "\n",
    "**What is the \"tidy data standard\" intended to accomplish?**\n",
    "\n",
    "The tidy data standard is intended to:\n",
    "\n",
    "1. **Facilitate initial data cleaning** - By providing a standard structure, you don't need to reinvent the wheel every time you clean data\n",
    "\n",
    "2. **Simplify data analysis** - When all datasets follow the same structure, tools work together seamlessly without needing translation between different formats\n",
    "\n",
    "3. **Enable tool development** - When both input and output are tidy, tools can be easily chained together\n",
    "\n",
    "4. **Focus on the interesting problem** - Less time spent on data manipulation logistics means more time for domain problems\n",
    "\n",
    "5. **Provide a common language** - Gives statisticians and data scientists a shared vocabulary for discussing data structure\n",
    "\n",
    "Ultimately, tidy data makes it easier to work with datasets by providing consistency and reducing the mental overhead of figuring out how each new dataset is organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.3: Section 2 Introduction\n",
    "\n",
    "**What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\"**\n",
    "\n",
    "This quote (adapted from Tolstoy's Anna Karenina) means that:\n",
    "\n",
    "- **Tidy datasets are all alike**: All tidy datasets follow the same three simple rules (variables in columns, observations in rows, observational units in tables). Once you understand these rules, you can work with any tidy dataset using the same set of tools.\n",
    "\n",
    "- **Every messy dataset is messy in its own way**: Messy datasets can violate the tidy principles in countless different ways - variables in rows, multiple variables in one column, observational units mixed together, etc. Each messy dataset requires custom logic to clean it.\n",
    "\n",
    "The implication is that standardizing on tidy data makes analysis easier because you only need to learn one structure, rather than dealing with endless variations of messy data.\n",
    "\n",
    "---\n",
    "\n",
    "**What does this sentence mean: \"For a given dataset, it's usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"**\n",
    "\n",
    "This means that:\n",
    "\n",
    "- **Easy for specific datasets**: When you look at a particular dataset (like medical records), it's usually clear what the variables (height, weight, age) and observations (individual patients) are in that context.\n",
    "\n",
    "- **Difficult to define generally**: But if you try to create a universal definition that works for ALL datasets, it becomes very difficult. For example:\n",
    "  - Are \"home phone\" and \"work phone\" two variables, or are they values of a \"phone type\" variable?\n",
    "  - Are \"height\" and \"width\" separate variables, or values of a \"dimension\" variable?\n",
    "  - The answer depends on your analysis goals and domain context\n",
    "\n",
    "The paper's rule of thumb: it's easier to describe relationships between variables (e.g., density = mass/volume) than between rows, and easier to compare groups of observations than groups of columns. This pragmatic approach helps you decide what should be a variable vs. an observation in ambiguous cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.4: Section 2.2 Definitions\n",
    "\n",
    "**How does Wickham define values, variables, and observations?**\n",
    "\n",
    "- **Values**: Numbers (if quantitative) or strings (if qualitative). Every value belongs to both a variable and an observation. Values are the actual data points in your dataset.\n",
    "\n",
    "- **Variables**: A variable contains all values that measure the same underlying attribute (like height, temperature, or duration) across units. All values in a variable measure the same thing across different observations.\n",
    "\n",
    "- **Observations**: An observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes. All values in an observation are about the same entity across different variables.\n",
    "\n",
    "**Example**: In a dataset about people:\n",
    "- \"height\" is a variable (measuring the same attribute across different people)\n",
    "- \"John Smith's measurements\" is an observation (measuring different attributes for the same person)\n",
    "- \"175\" (John's height in cm) is a value that belongs to both the \"height\" variable and the \"John Smith\" observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.5: Section 2.3\n",
    "\n",
    "**How is \"Tidy Data\" defined?**\n",
    "\n",
    "Tidy data is defined by three characteristics:\n",
    "\n",
    "1. **Each variable forms a column** - Every column represents one and only one variable\n",
    "\n",
    "2. **Each observation forms a row** - Every row represents one and only one observation\n",
    "\n",
    "3. **Each type of observational unit forms a table** - Different types of entities are stored in different tables\n",
    "\n",
    "This is essentially **Codd's 3rd normal form** from database theory, but expressed in statistical language and focused on single datasets rather than the many connected datasets common in relational databases.\n",
    "\n",
    "Any other arrangement of data is considered \"messy.\"\n",
    "\n",
    "The beauty of this definition is its simplicity - just three rules that, when followed, make data analysis dramatically easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.6: Section 3 and 3.1\n",
    "\n",
    "**What are the 5 most common problems with messy datasets?**\n",
    "\n",
    "1. **Column headers are values, not variable names** - e.g., years or income brackets as column headers instead of a single \"year\" or \"income\" variable\n",
    "\n",
    "2. **Multiple variables are stored in one column** - e.g., \"m25-34\" combining sex and age group in one field\n",
    "\n",
    "3. **Variables are stored in both rows and columns** - e.g., temperature measurements where variable names (tmin, tmax) are values in an \"element\" column\n",
    "\n",
    "4. **Multiple types of observational units are stored in the same table** - e.g., song metadata and weekly rankings mixed together\n",
    "\n",
    "5. **A single observational unit is stored in multiple tables** - e.g., data split across multiple files by year or location\n",
    "\n",
    "---\n",
    "\n",
    "**Why are the data in Table 4 messy?**\n",
    "\n",
    "Table 4 (the Pew religion and income data) is messy because:\n",
    "\n",
    "- **Column headers are values, not variable names**: The columns (<$10k, $10-20k, $20-30k, etc.) are values of an \"income\" variable, not variable names themselves\n",
    "\n",
    "- **This creates three problems**:\n",
    "  1. The income variable is spread across multiple columns instead of being in one column\n",
    "  2. To access income data, you need different code for each income bracket\n",
    "  3. It's difficult to do operations that involve all income values (like computing means or creating visualizations)\n",
    "\n",
    "The tidy version (Table 6) has three columns (religion, income, freq) where income is a proper variable with all income brackets as values in that single column. This makes it easy to filter by income bracket, calculate statistics, or create plots.\n",
    "\n",
    "---\n",
    "\n",
    "**What is \"melting\" a dataset?**\n",
    "\n",
    "Melting (also called \"stacking\") is the operation of turning columns into rows.\n",
    "\n",
    "**How it works:**\n",
    "- You specify which columns should remain as they are (colvars - column variables)\n",
    "- The other columns get \"melted\" into two new columns:\n",
    "  - A \"column\" variable (containing the old column names)\n",
    "  - A \"value\" variable (containing the data from those columns)\n",
    "\n",
    "**Example:** Table 5 shows melting where:\n",
    "- \"row\" is the colvar (stays as a column)\n",
    "- Columns a, b, c become values in a new \"column\" variable\n",
    "- The data from a, b, c becomes values in a new \"value\" variable\n",
    "\n",
    "Melting is the key operation for fixing Problem #1 (column headers are values). In pandas, this is implemented as `pd.melt()` or `DataFrame.melt()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.7: Tables 11 and 12\n",
    "\n",
    "**Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?**\n",
    "\n",
    "**Table 11 is messy because:**\n",
    "- **Variables are stored in both rows AND columns**:\n",
    "  - The variable names (tmax, tmin) are stored as values in the \"element\" column (rows)\n",
    "  - The day of month is spread across columns (d1-d31)\n",
    "  - Temperature values are scattered across the table in a way that makes them hard to access\n",
    "  \n",
    "**Table 12(b) is tidy because:**\n",
    "- **After melting** (Table 12a): Days are converted from columns to a \"date\" column\n",
    "- **After casting**: The \"element\" values (tmax/tmin) are rotated back out into proper column variables\n",
    "- **Final result**: \n",
    "  - Each variable has its own column (id, date, tmax, tmin)\n",
    "  - Each row represents one day's observation\n",
    "  - All temperature data is easily accessible using standard column operations\n",
    "\n",
    "The transformation requires both:\n",
    "1. **Melting** to fix the days-as-columns problem (turning d1-d31 into date values)\n",
    "2. **Casting** to fix the variable-names-as-values problem (turning element values into proper columns)\n",
    "\n",
    "This example shows how the same data can be structured in fundamentally different ways, and why the tidy structure makes analysis so much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.8: Section 6\n",
    "\n",
    "**What is the \"chicken-and-egg\" problem with focusing on tidy data?**\n",
    "\n",
    "The chicken-and-egg problem is:\n",
    "\n",
    "- **Tidy data is only useful if you have tidy tools to work with it**\n",
    "- **But tidy tools are only useful if you have tidy data to process**\n",
    "\n",
    "This creates a **local maxima problem** where:\n",
    "- You can't improve data structures without updating all your tools\n",
    "- You can't improve tools without changing your data structures\n",
    "- Independently changing either one doesn't improve your workflow\n",
    "- You need coordinated, long-term effort to change both together\n",
    "\n",
    "This makes it hard to know if tidy data is actually the best solution, or just one local optimum. We might be stuck in a pattern that works okay but isn't actually optimal. Breaking out of this requires \"long-term concerted effort with the prospect of many false starts.\"\n",
    "\n",
    "---\n",
    "\n",
    "**What does Wickham hope happens in the future with further work on the subject of data wrangling?**\n",
    "\n",
    "Wickham hopes that:\n",
    "\n",
    "1. **Alternative formulations of tidiness** will be explored - perhaps array-based tidy formats for multidimensional data (like microarrays or fMRI data) that are more memory-efficient and can leverage matrix operations\n",
    "\n",
    "2. **Research from other fields** (human factors, user-centered design, HCI) will contribute to understanding the cognitive aspects of data analysis. How do people actually think about data?\n",
    "\n",
    "3. **User testing and empirical methods** (ethnography, talk-aloud protocols, user testing) will be used to better understand how people work with data in practice, not just in theory\n",
    "\n",
    "4. **Better data storage strategies** will be developed through building on this framework, potentially addressing the chicken-and-egg problem\n",
    "\n",
    "5. **The conversation will continue** - Wickham sees tidy data as a starting point, not the final solution. He hopes others will build on this work to develop even better approaches\n",
    "\n",
    "He acknowledges that tidy data might itself be a \"false start\" but hopes it will at least inspire better solutions. His humble approach shows good scientific thinking - proposing ideas while remaining open to better alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This homework explored fundamental data wrangling concepts:\n",
    "\n",
    "**Q1 - Data Cleaning:**\n",
    "- Cleaned numeric (Price), categorical (subject_injury), and dummy (pretrial release) variables\n",
    "- Learned to identify and handle different types of missing data (MCAR, MAR, MNAR)\n",
    "- Discovered that understanding *why* data is missing is crucial for proper analysis\n",
    "\n",
    "**Q2 - Shark Attacks:**\n",
    "- (To be completed with data file)\n",
    "\n",
    "**Q3 - Tidy Data Principles:**\n",
    "- Learned the three principles of tidy data\n",
    "- Understood why tidy data makes analysis easier\n",
    "- Explored common problems with messy data and how to fix them\n",
    "- Recognized that data structure matters as much as data content\n",
    "\n",
    "**Key Takeaway**: Good data wrangling isn't just about cleaning data - it's about structuring data in ways that make analysis natural and intuitive. The tidy data framework provides a systematic approach to this challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
